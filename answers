1. The decision tree was a binary tree represented as an array. Before cleaning or pruning, the tree is a full binary tree, where the left subtree of a node at index i is located at 2*i, and the right subtree is located at index 2*i + 1. Every level of the tree doubles the size of the array, so it is recommended to specify a maximum number of levels in the tree. The values held at each node included a count of class '0' testing samples associated with the node, class '1' testing samples associated with the node. Also included is the pivot about which the training data was split, the value used in the split, the attribute used in the split, and a value indicating whether the node is a leaf.

2. The data from the input files is read in as a matrix, with columns representing attributes and rows representing each example. Part of the matrix is sorted about a row as a pivot.

3. Each node iterates over a list of available attributes (all attributes if reuse attributes is enabled, or else a subset). For each attribute, the best entropy is calculated by iterating over the sorted training data to find the split providing the smallest entropy. This takes O(n + n log n) time for each attribute, or O(n k log n) where n is the number of training examples associated with a node and k is the number of attributes.

4. Missing attributes are dealt with at the beginning (not on a per node basis). At first, the average for each attribute is calculated across all training examples (where known). The appropriate average is then assigned to each unknown value. An alternative method looking at the majority at each node was also considered.

5. Termination criteria vary for pruned vs. unpruned trees. If pruning is used, termination can occur if certain thresholds used in pruning are met (for pruning occuring during tree generation). Otherwise, termination occurs when a maximum number of levels is reached (without attribute reuse, this can be no higher than the number of attributes).

6. (tree may be output, but output into DNF is not complete yet)

7. (Rule as printed by program)
|||= Split on attribute  opprundifferential < 27.0 into left subtree, else right subtree
(english)
This is a 4th level node, but not a leaf. Check the test example's value for the opprundifferential, and continue the test using the left subtree if this value is less than 27.0. Otherwise, continue the test in the right subtree.

8. Pruning is implemented as a variant on reduced error pruning. A greedy strategy is used during tree generation, with two methods used. If the ratio of one class  to another is greater than a threshold, no subtrees are generated, the majority class is used as the estimate, and the node becomes a leaf. If there are relatively few examples associated with a node (compared to the overall number of training examples), then no subtrees are generated, the majority class for that node is used, and the node becomes a leaf. In addition the tree is trimmed from the bottom up in both pruned and unpruned trees to collapse redundant sibling leaves into their parent nodes.

9. (tree may be output, but output into DNF is not complete yet)

10.

