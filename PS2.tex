
\documentclass[11pt]{article} 
\usepackage[utf8]{inputenc} 

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% END Article customizations

\usepackage{listings}

%%% The "real" document content comes below...

\title{EECS 349 PS2}
\author{Christopher Walker}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{}

The decision tree was a binary tree represented as an array. Before cleaning or pruning, the tree is a full binary tree, where the left subtree of a node at index $i$ is located at $2i$, and the right subtree is located at index $2i + 1$. Every level of the tree doubles the size of the array, so it is recommended to specify a maximum number of levels in the tree. The values held at each node included a count of class '0' testing samples associated with the node, class '1' testing samples associated with the node. Also included is the pivot about which the training data was split, the value used in the split, the attribute used in the split, and a value indicating whether the node is a leaf.

\section{}

The data from the input files is read in as a matrix, with columns representing attributes and rows representing each example. Part of the matrix is sorted about a row as a pivot.

\section{}

Each node iterates over a list of available attributes (all attributes if reuse attributes is enabled, or else a subset). For each attribute, the best entropy is calculated by iterating over the sorted training data to find the split providing the smallest entropy. This takes $O(n + n \log n)$ time for each attribute, or $O(n k \log n)$ where $n$ is the number of training examples associated with a node and k is the number of attributes.

\section{}

Missing attributes are dealt with at the beginning (not on a per node basis). At first, the average for each attribute is calculated across all training examples (where known). The appropriate average is then assigned to each unknown value. An alternative method looking at the majority at each node was also considered.

\section{}

Termination criteria vary for pruned vs. unpruned trees. If pruning is used, termination can occur if certain thresholds used in pruning are met (for pruning occuring during tree generation). Otherwise, termination occurs when a maximum number of levels is reached (without attribute reuse, this can be no higher than the number of attributes).

\section{}

(Output of an unpruned 5-level tree after cleaning)
\begin{lstlisting}
if ( numinjured < 1.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent < 0.339861438569)AND( opprundifferential < 30.0)
    AND(winpercent < 0.645427521528) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent < 0.339861438569)AND( opprundifferential < 30.0)AND
    (winpercent >= 0.645427521528) then class 0
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent < 0.339861438569)AND( opprundifferential >= 30.0) 
    then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent >= 0.339861438569)AND( opprundifferential < 49.0)
    then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent >= 0.339861438569)AND( opprundifferential >= 49.0)
    AND( rundifferential < 25.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent >= 0.339861438569)AND( opprundifferential >= 49.0)
    AND( rundifferential >= 25.0) then class 0
\end{lstlisting}

\section{}

(Rule as printed by program, using per-node printing rather than DNF format)
\begin{lstlisting}
|||= Split on attribute  opprundifferential < 27.0 into left subtree,
    else right subtree
\end{lstlisting}
(english)
This is a 4th level node, but not a leaf. Check the test example's value for the opprundifferential, and continue the test using the left subtree if this value is less than 27.0. Otherwise, continue the test in the right subtree.

\section{}

Pruning is implemented as a variant on reduced error pruning. A greedy strategy is used during tree generation, with two methods used. If the ratio of one class  to another is greater than a threshold, no subtrees are generated, the majority class is used as the estimate, and the node becomes a leaf. If there are relatively few examples associated with a node (compared to the overall number of training examples), then no subtrees are generated, the majority class for that node is used, and the node becomes a leaf. In addition the tree is trimmed from the bottom up in both pruned and unpruned trees to collapse redundant sibling leaves into their parent nodes.

\section{}

(Output of a 12-level aggressively pruned tree after cleaning)
\begin{lstlisting}
if ( numinjured < 1.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0)AND
    ( oppwinpercent < 0.399827683029) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0)AND
    ( oppwinpercent >= 0.399827683029)AND
    ( opprundifferential < 109.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0)AND
    ( oppwinpercent >= 0.399827683029)AND
    ( opprundifferential >= 109.0)AND( rundifferential < 27.0)
    AND(winpercent < 0.590400253747) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0)AND
    ( oppwinpercent >= 0.399827683029)AND( opprundifferential >= 109.0)
    AND( rundifferential < 27.0)AND(winpercent >= 0.590400253747)
    then class 0
if ( numinjured >= 1.0)AND( oppnuminjured < 3.0)AND
    ( oppwinpercent >= 0.399827683029)AND
    ( opprundifferential >= 109.0)AND( rundifferential >= 27.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent < 0.339861438569) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)
    AND( oppwinpercent >= 0.339861438569)AND
    ( opprundifferential < 49.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent >= 0.339861438569)AND
    ( opprundifferential >= 49.0)AND( rundifferential < 25.0) then class 1
if ( numinjured >= 1.0)AND( oppnuminjured >= 3.0)AND
    ( oppwinpercent >= 0.339861438569)AND( opprundifferential >= 49.0)
    AND( rundifferential >= 25.0) then class 0
\end{lstlisting}

\section{}

Size of an aggresively pruned 12-level tree after cleaning is 19 and 39 before cleaning, while size of a 6-level unpruned tree is 55 after cleaning and 127 before cleaning. The size of the trees varies after the number of levels is changed, after cleaning, and depends on pruning parameters.

\section{}

Accuracy of a cleaned unpruned tree with 6 levels is 0.60996. Accuracy of an aggresively pruned 12-level tree is 0.62546. More attributes were kept in the 6-level unpruned tree, but they were not necessarily helpful in improving accuracy.

\section{}

(Learning curves still being created)

\section{}

The pruned tree is currently expected to perform better, because it does not overfit as much and sticks to the attributes providing best entropy results.

\section{}

Temperature and weather appeared to be poor fits for the training set. In a baseball game, these represent factors which would affect both teams in a similar way. Starting pitcher and oppstartingpitcher also appeared to yield poor results. These gave higher entropy splits and tended to appear in lower levels of the tree rather than higher levels. This may be because a starting pitcher's utility is useful relative to the other team's pitchers. Looking at each team's pitchers individually may be less useful than looking at a function of the two team's pitchers. Numinjured and oppnuminjured seemed to provide the highest entropy splits, and were often near the top of the tree. Unfortunately, this made reuse attribute trees less useful because numinjured was overused. As with startingpitcher and oppstartingpitcher, looking at a function of numinjured and oppnuminjured might be even better than looking at numinjured or oppnuminjured alone. Along the same lines it may make sense to examine a function of rundifferential and opprundifferential, as well as dayssincegame and oppdayssincegame (the difference may be more important than the attributes themselves).

\section{}

Eric worked on learning curve, classification, validation, command line parsing, and tree generation. I worked on input, command line parsing, tree generation, tree printing, pruning, cleaning, testing


\end{document}
